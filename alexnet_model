learning_rate = 0.001
epochs = 5000
batch_size = 128
display_step = 200
height = 227
width = 227
n_channels = 3
n_classes = 30
X = tf.placeholder(tf.float32, shape = [None, height, width, n_channels])
Y = tf.placeholder(tf.float32, shape = [None, n_classes])

weights = {
    # Convolutional Layer 1: 11x11 filters, 3 input channels, 96 output channels
    'w1' : tf.Variable(tf.random_normal([11, 11, 3, 96])), 
    # Convolutional Layer 2: 5x5 filters, 96 input channels, 256 output channels
    'w2' : tf.Variable(tf.random_normal([5, 5, 96, 256])),
    # Convolutional Layer 3: 3x3 filters, 256 input channels, 384 output channels
    'w3' : tf.Variable(tf.random_normal([3, 3, 256, 384])),
    # Convolutional Layer 4: 3x3 filters, 384 input channels, 384 output channels
    'w4' : tf.Variable(tf.random_normal([3, 3, 384, 384])),
    # Convolutional Layer 5: 3x3 filters, 384 input channels, 256 output channels
    'w5' : tf.Variable(tf.random_normal([3, 3, 384, 256])),
    # Fully Connected Layer 1: 9216 input channels, 4096 output channels
    'w6' : tf.Variable(tf.random_normal([9216, 4096])),
    # Fully Connected Layer 2: 4096 input channels, 4096 output channels
    'w7' : tf.Variable(tf.random_normal([4096, 4096])),
    # Fully Connected Layer 3: 4096 input channels, 30(number of classes) output channels
    'w8' : tf.Variable(tf.random_normal([4096, n_classes]))
}
biases = {
    'b1' : tf.Variable(tf.random_normal([96])),
    'b2' : tf.Variable(tf.random_normal([256])),
    'b3' : tf.Variable(tf.random_normal([384])),
    'b4' : tf.Variable(tf.random_normal([384])),
    'b5' : tf.Variable(tf.random_normal([256])),
    'b6' : tf.Variable(tf.random_normal([4096])),
    'b7' : tf.Variable(tf.random_normal([4096])),
    'b8' : tf.Variable(tf.random_normal([n_classes]))
}
def conv2d(x, W, b, strides = 1, padding = 'SAME'):
    x = tf.nn.conv2d(x, W, strides = [1, strides, strides, 1], padding = padding)
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)
def maxpool2d(x, k = 2, padding = 'VALID'):
    return tf.nn.max_pool(x, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = padding)
def alexnet(x, w, b):
    x = tf.reshape(x, shape = [-1, 227, 227, 3])
    
    # Layer 1
    conv1 = conv2d(x, w['w1'], b['b1'], strides = 4, padding = 'VALID') # Convolution
    conv1 = maxpool2d(conv1) # Pooling
    
    # Layer 2
    conv2 = conv2d(conv1, w['w2'], b['b2']) # Convolution
    conv2 = maxpool2d(conv2) # Pooling
    
    # Layer 3
    conv3 = conv2d(conv2, w['w3'], b['b3']) # Convolution
    
    # Layer 4
    conv4 = conv2d(conv3, w['w4'], b['b4']) # Convolution
    
    # Layer 5
    conv5 = conv2d(conv4, w['w5'], b['b5']) # Convolution
    conv5 = maxpool2d(conv5) # Pooling
    
    # Layer 6
    fc1 = tf.reshape(conv5, [-1, weights['w6'].get_shape().as_list()[0]]) # Channel Reshape
    fc1 = tf.add(tf.matmul(fc1, w['w6']), b['b6']) # Linear Function
    fc1 = tf.nn.relu(fc1) # Activation Function

    # Layer 7
    fc2 = tf.add(tf.matmul(fc1, w['w7']), b['b7']) # Linear Function
    fc2 = tf.nn.relu(fc2) # Activation Function
    
    # Layer 8
    out = tf.add(tf.matmul(fc2, w['w8']), b['b8']) # Linear Function
    
    return out
    logits = alexnet(X, weights, biases)
loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))
optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)
train_op = optimizer.minimize(loss_op)

correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
accuracy
